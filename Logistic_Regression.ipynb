{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Logistic Regression Assignment**\n",
        "\n",
        "1. What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        ">> Logistic Regression is a statistical model used for binary classification tasks (predicting one of two classes). It differs from Linear Regression primarily in its output: Linear Regression predicts a continuous numerical value, while Logistic Regression outputs a probability (between 0 and 1) that an instance belongs to a particular class, which is then thresholded to make a class prediction. Linear Regression uses a linear equation, while Logistic Regression uses the sigmoid (logistic) function to transform the linear output into a probability.\n",
        "\n",
        "2. What is the mathematical equation of Logistic Regression?\n",
        " >> The core of the logistic regression model is: P(Y=1|X) = 1 / (1 + e^(-(b0 + b1x1 + ... + bnxn))). This can also be written as y_hat = sigma(w^T x + b), where sigma(z) = 1 / (1 + e^(-z)) is the sigmoid function, w is the vector of weights, x is the vector of input features, and b is the bias (intercept).\n",
        "\n",
        "3. Why do we use the Sigmoid function in Logistic Regression?\n",
        " >> The Sigmoid function (also known as the logistic function) is used in Logistic Regression because it maps any real-valued number into a value between 0 and 1. This property makes it ideal for representing probabilities. The output of the linear combination of features and weights can be any real number, but we need a probability for classification, so the sigmoid function transforms this linear output into a probability.\n",
        "\n",
        "4. What is the cost function of Logistic Regression?\n",
        "\n",
        " >> The most common cost function for Logistic Regression is the Log Loss (also known as Binary Cross-Entropy).\n",
        "  For a single training example (x^(i), y^(i)), the cost is:\n",
        "  If y^(i) = 1: -log(y_hat^(i))\n",
        "  If y^(i) = 0: -log(1 - y_hat^(i))\n",
        "  The overall cost function for m training examples is: J(w, b) = -(1/m) * sum_{i=1 to m} [y^(i) * log(y_hat^(i)) + (1 - y^(i)) * log(1 - y_hat^(i))]\n",
        "\n",
        "5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        ">> Regularization in Logistic Regression is a technique used to prevent overfitting. It works by adding a penalty term to the cost function, which discourages the model from assigning excessively large weights to the features. It is needed because without it, a complex model might learn the noise in the training data too well, leading to poor generalization on unseen data.\n",
        "\n",
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
        ">>\n",
        "* **Lasso (L1 Regularization):** Adds a penalty proportional to the absolute value of the coefficients (||w||_1 = sum |w_i|). It has a feature selection property, meaning it can drive some coefficients exactly to zero, effectively removing those features from the model.\n",
        "* **Ridge (L2 Regularization):** Adds a penalty proportional to the square of the magnitude of the coefficients (||w||_2^2 = sum w_i^2). It shrinks coefficients towards zero but rarely makes them exactly zero. It helps in reducing multicollinearity.\n",
        "* **Elastic Net:** Is a hybrid of Lasso and Ridge regularization. It adds both L1 and L2 penalty terms to the cost function. It is particularly useful when there are highly correlated features, as it can select groups of correlated variables.\n",
        "\n",
        "7. When should we use Elastic Net instead of Lasso or Ridge?\n",
        "\n",
        " >> You should use Elastic Net when:\n",
        "    * You have a large number of features and expect only a subset to be truly relevant (Lasso's strength).\n",
        "    * You have groups of highly correlated features, and you want to select all of them together rather than just one (where Lasso might arbitrarily pick one).\n",
        "    * You want the regularization properties of both L1 and L2 (feature selection and coefficient shrinkage/multicollinearity handling).\n",
        "    * Lasso performs poorly due to strong correlations between predictors.\n",
        "\n",
        "8. What is the impact of the regularization parameter (lambda) in Logistic Regression?\n",
        "\n",
        ">> The regularization parameter (lambda, often denoted as alpha or C in libraries like scikit-learn, where C = 1/lambda) controls the strength of the regularization.\n",
        "  * **Large lambda (or small C):** Increases the penalty, leading to smaller coefficients and a simpler model, potentially causing underfitting.\n",
        "  * **Small lambda (or large C):** Decreases the penalty, allowing coefficients to be larger, leading to a more complex model, potentially causing overfitting.\n",
        "The optimal lambda value is typically found through cross-validation.\n",
        "\n",
        "9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        ">> Key assumptions of Logistic Regression include:\n",
        "  * **Binary Outcome:** The dependent variable must be binary or dichotomous.\n",
        "  * **Independence of Observations:** Observations should be independent of each other.\n",
        "  * **No Multicollinearity:** Independent variables should not be highly correlated with each other.\n",
        "  * **Linearity of Log-Odds:** The relationship between the independent variables and the log-odds of the dependent variable is linear.\n",
        "  * **Large Sample Size:** Logistic Regression performs better with larger sample sizes.\n",
        "\n",
        "10. What are some alternatives to Logistic Regression for classification tasks?\n",
        " >> Some alternatives to Logistic Regression for classification tasks include:\n",
        "    * Support Vector Machines (SVMs)\n",
        "    * Decision Trees\n",
        "    * Random Forests\n",
        "    * Gradient Boosting Machines (e.g., XGBoost, LightGBM)\n",
        "    * K-Nearest Neighbors (KNN)\n",
        "    * Naive Bayes\n",
        "    * Neural Networks\n",
        "\n",
        "11. What are Classification Evaluation Metrics?\n",
        ">> Classification evaluation metrics are used to quantify the performance of a classification model. Common metrics include:\n",
        "    * **Accuracy:** Proportion of correctly classified instances.\n",
        "    * **Precision:** Proportion of true positives among all predicted positives.\n",
        "    * **Recall (Sensitivity):** Proportion of true positives among all actual positives.\n",
        "    * **F1-Score:** Harmonic mean of Precision and Recall.\n",
        "    * **ROC Curve and AUC (Area Under the Receiver Operating Characteristic Curve):** Evaluates the model's ability to distinguish between classes at various threshold settings.\n",
        "    * **Confusion Matrix:** A table summarizing true positives, true negatives, false positives, and false negatives.\n",
        "    * **Log Loss (Binary Cross-Entropy):** Measures the performance of a classification model where the prediction input is a probability value between 0 and 1.\n",
        "\n",
        "12. How does class imbalance affect Logistic Regression?\n",
        "\n",
        ">> Class imbalance occurs when one class significantly outnumbers the other(s) in the training data. In Logistic Regression, this can lead to:\n",
        "  * **Biased Model:** The model might become biased towards the majority class, as it will achieve high accuracy simply by predicting the majority class for most instances.\n",
        "  * **Poor Performance on Minority Class:** The model might perform very poorly in identifying the minority class, as it has seen fewer examples of it.\n",
        "  * **Misleading Accuracy:** High accuracy can be misleading if the minority class is the one of interest.\n",
        "To address this, techniques like oversampling the minority class, undersampling the majority class, using synthetic minority oversampling technique (SMOTE), or using class weights in the loss function are employed.\n",
        "\n",
        "13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        ">> Hyperparameter tuning in Logistic Regression involves finding the optimal values for the hyperparameters of the model to achieve the best performance. For Logistic Regression, the main hyperparameter is the regularization parameter (lambda or C). Other potential hyperparameters might include the solver used or the maximum number of iterations. This is typically done using techniques like Grid Search, Random Search, or Bayesian Optimization.\n",
        "\n",
        "14. What are different solvers in Logistic Regression? Which one should be used?\n",
        ">> Common solvers used in Logistic Regression (e.g., in scikit-learn) include:\n",
        "    * **liblinear:** Good for small datasets, supports L1 and L2 regularization.\n",
        "    * **newton-cg:** Good for large datasets, supports only L2 regularization, handles multiclass.\n",
        "    * **lbfgs:** Default in scikit-learn, good for large datasets, supports only L2 regularization, handles multiclass.\n",
        "    * **sag (Stochastic Average Gradient):** Good for very large datasets, faster for large datasets than lbfgs and newton-cg, supports only L2 regularization.\n",
        "    * **saga (Stochastic Average Gradient descent with Proximal Operator):** A variant of sag that also supports L1 regularization, making it suitable for sparse models.\n",
        "The choice of solver depends on the dataset size, the type of regularization needed (L1, L2, or none), and whether it's a binary or multiclass problem. For most general cases, `lbfgs` is a good default. For very large datasets, `sag` or `saga` are preferred. If L1 regularization is required, `liblinear` or `saga` must be used.\n",
        "\n",
        "15. How is Logistic Regression extended for multiclass classification?\n",
        "\n",
        ">> Logistic Regression can be extended for multiclass classification using two main strategies:\n",
        "  * **One-vs-Rest (OvR) or One-vs-All (OvA):** Trains a separate binary Logistic Regression model for each class. Each model distinguishes one class from all the others. For prediction, the class with the highest predicted probability (or score) from its respective model is chosen.\n",
        "  * **Multinomial Logistic Regression (Softmax Regression):** This is a direct extension that models the probability of each class directly, rather than building separate binary classifiers. It uses the Softmax function in the output layer, which normalized the outputs into a probability distribution over all classes.\n",
        "\n",
        "16. What are the advantages and disadvantages of Logistic Regression?\n",
        ">>\n",
        "    **Advantages:**\n",
        "    * Simple and easy to implement and interpret.\n",
        "    * Outputs probabilities, which can be useful for decision-making.\n",
        "    * Computationally efficient.\n",
        "    * Less prone to overfitting than more complex models if regularization is applied.\n",
        "    * Good baseline model for classification tasks.\n",
        "\n",
        "    **Disadvantages:**\n",
        "    * Assumes a linear relationship between independent variables and the log-odds of the dependent variable, which might not always hold true.\n",
        "    * Can suffer from multicollinearity if not handled.\n",
        "    * Not suitable for highly complex relationships or non-linear decision boundaries.\n",
        "    * Can be sensitive to outliers.\n",
        "    * Performance can degrade with a large number of features, especially if many are irrelevant.\n",
        "\n",
        "17. What are some use cases of Logistic Regression?\n",
        ">>\n",
        "    * **Email Spam Detection:** Classifying emails as spam or not spam.\n",
        "    * **Disease Prediction:** Predicting the likelihood of a patient having a certain disease (e.g., diabetes, heart disease).\n",
        "    * **Customer Churn Prediction:** Identifying customers likely to cancel a subscription or service.\n",
        "    * **Credit Score Prediction:** Assessing the probability of loan default.\n",
        "    * **Marketing Campaign Success:** Predicting whether a customer will click on an ad or make a purchase.\n",
        "    * **Fraud Detection:** Identifying fraudulent transactions.\n",
        "\n",
        "18. What is the difference between Softmax Regression and Logistic Regression? >>\n",
        "  * **Logistic Regression:** Primarily used for **binary classification** tasks (two classes). It uses the sigmoid function to output a probability between 0 and 1.\n",
        "  * **Softmax Regression (Multinomial Logistic Regression):** Used for **multiclass classification** tasks (more than two classes). It uses the softmax function, which outputs a probability distribution over all possible classes. The sum of these probabilities for a given instance will always be 1. Softmax Regression can be seen as a generalization of Logistic Regression for multiple classes.\n",
        "\n",
        "19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        ">>\n",
        "    * **Theoretical Perspective:** Softmax Regression is generally preferred when the classes are mutually exclusive (an instance belongs to only one class), as it directly models the probabilities of each class. OvR, on the other hand, assumes that each class can be independently classified from the others, which might not always be ideal if there are strong relationships or overlaps between classes.\n",
        "    * **Practical Implementation:**\n",
        "        * **OvR:** Often simpler to implement, as it reuses existing binary classifiers. It can be more robust if there are errors in the training data leading to overlapping classes.\n",
        "        * **Softmax:** More elegant and often leads to better performance when the mutual exclusivity assumption holds well.\n",
        "    * **Performance:** For many practical scenarios, the performance difference might not be significant, especially with well-separated classes. However, Softmax is generally considered more theoretically sound for mutually exclusive multiclass problems. Many libraries default to OvR for its simplicity unless explicitly specified to use Softmax.\n",
        "\n",
        "20. How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        ">> In Logistic Regression, the coefficients (beta_i) are interpreted in terms of **log-odds**.\n",
        "  * For a one-unit increase in the independent variable x_i, while holding all other variables constant, the **log-odds** of the dependent variable being 1 (the event occurring) change by beta_i.\n",
        "  * Exponentiating the coefficient, e^(beta_i), gives the **odds ratio**. This means that for a one-unit increase in x_i, the odds of the event occurring are multiplied by e^(beta_i).\n",
        "  * If beta_i > 0: An increase in x_i increases the log-odds (and thus the probability) of the event.\n",
        "  * If beta_i < 0: An increase in x_i decreases the log-odds (and thus the probability) of the event.\n",
        "  * If beta_i = 0: x_i has no effect on the log-odds (or probability) of the event.\n",
        "It's important to remember that the relationship is non-linear with respect to the probability itself, but linear with respect to the log-odds."
      ],
      "metadata": {
        "id": "f66l5TLu_I5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "Rn39Hsu2_KFf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic Regression, and prints the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data= load_breast_cancer()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3B55BdiQCwTe",
        "outputId": "5d4ab3d4-d8d1-4668-91ca-dfce2d7f07a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.9790209790209791\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data= load_breast_cancer()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression(penalty='l1', solver=\"saga\", max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zgzrp3FXDeSN",
        "outputId": "13550285-2401-4e6e-833c-610bf98e4ac1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.986013986013986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data= load_iris()\n",
        "df= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "df[\"target\"]= data.target\n",
        "df=df[df[\"target\"] != 2]\n",
        "\n",
        "X= df.drop(\"target\", axis=1)\n",
        "y= df.target\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression(penalty='l2', max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "\n",
        "for i in range(len(df.columns[:-1])):\n",
        "  print(f\"Coefficient of {df.columns[i]} is {model.coef_[0][i]}\")\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pV7XezGBDmjw",
        "outputId": "2feb4bc5-400d-4006-d027-3036b9a89340"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coefficient of sepal length (cm) is 0.8472573055246394\n",
            "Coefficient of sepal width (cm) is -1.0303886088356948\n",
            "Coefficient of petal length (cm) is 1.4353226935010241\n",
            "Coefficient of petal width (cm) is 1.4353967055089638\n",
            "Accuracy Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet').\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data= load_breast_cancer()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression(penalty='elasticnet', solver=\"saga\", l1_ratio=0.25,max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qg_DEBRTI97X",
        "outputId": "a1450f22-62ac-4085-c5f9-46089ff0de2f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.986013986013986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "data= load_iris()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression(multi_class=\"ovr\")\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_s1M2nKOuu",
        "outputId": "d34cf751-1715-4073-95b4-747aff89defd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.8947368421052632\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "data= load_iris()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "params={\"penalty\":[\"l1\", \"l2\"], \"C\":[0.1, 0.5,1,2,5, 10]}\n",
        "\n",
        "grid_log = GridSearchCV(LogisticRegression(max_iter=1000), cv=5, param_grid=params, verbose=2)\n",
        "grid_log.fit(X_train, y_train)\n",
        "\n",
        "y_pred=grid_log.best_estimator_.predict(X_test)\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "print(\"\\nBest parameters after tuning are:\", grid_log.best_estimator_)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CZwbSfM-M5_C",
        "outputId": "c7ddbe71-9e7a-4290-af00-67faa311525f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
            "[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.1, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n",
            "[CV] END ..................................C=0.1, penalty=l2; total time=   0.1s\n",
            "[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n",
            "[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n",
            "[CV] END ..................................C=0.1, penalty=l2; total time=   0.1s\n",
            "[CV] END ..................................C=0.5, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.5, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.5, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.5, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.5, penalty=l1; total time=   0.0s\n",
            "[CV] END ..................................C=0.5, penalty=l2; total time=   0.1s\n",
            "[CV] END ..................................C=0.5, penalty=l2; total time=   0.0s\n",
            "[CV] END ..................................C=0.5, penalty=l2; total time=   0.0s\n",
            "[CV] END ..................................C=0.5, penalty=l2; total time=   0.0s\n",
            "[CV] END ..................................C=0.5, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.1s\n",
            "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.1s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.1s\n",
            "[CV] END ....................................C=2, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=5, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=5, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=5, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=5, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=5, penalty=l1; total time=   0.0s\n",
            "[CV] END ....................................C=5, penalty=l2; total time=   0.0s\n",
            "[CV] END ....................................C=5, penalty=l2; total time=   0.1s\n",
            "[CV] END ....................................C=5, penalty=l2; total time=   0.3s\n",
            "[CV] END ....................................C=5, penalty=l2; total time=   0.1s\n",
            "[CV] END ....................................C=5, penalty=l2; total time=   0.1s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l1; total time=   0.0s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.4s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.2s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.3s\n",
            "[CV] END ...................................C=10, penalty=l2; total time=   0.1s\n",
            "\n",
            "Best parameters after tuning are: LogisticRegression(C=2, max_iter=1000)\n",
            "Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
        "\n",
        "\n",
        "data= load_iris()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression(max_iter=1000)\n",
        "\n",
        "cvs=cross_val_score(model, X_train, y_train, cv=StratifiedKFold(n_splits=5))\n",
        "print(\"Average accuracy for the Stratified K Fold cross validation is\", np.mean(cvs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QUTf5C0OXso",
        "outputId": "8547165c-7ce6-479b-ca53-a4b1d83f5983"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average accuracy for the Stratified K Fold cross validation is 0.9640316205533598\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "df=pd.read_csv(\"/content/logisitic_test.csv\")\n",
        "X=df.drop(\"Purchased\", axis=1)\n",
        "y=df.Purchased\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression(max_iter=10000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApvMBe_iR0jd",
        "outputId": "43fc737e-7aae-4740-a65c-05036c7bddd9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#9.   Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in Logistic Regression. Print the best parameters and accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "\n",
        "data= load_iris()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "params={\"penalty\":[\"l1\", \"l2\"], \"C\":[0.1, 0.5,1,2,5, 10], \"solver\":[\"saga\", \"liblinear\", \"lbfgs\", \"sag\"]}\n",
        "\n",
        "random_log =RandomizedSearchCV(LogisticRegression(max_iter=1000), cv=5, param_distributions=params, n_iter=3, verbose=2)\n",
        "random_log.fit(X_train, y_train)\n",
        "\n",
        "y_pred=random_log.best_estimator_.predict(X_test)\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "print(\"\\nBest parameters after tuning are:\", random_log.best_estimator_)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iOd8M2HSjVD",
        "outputId": "abb35e14-7e4c-4275-cbe4-aee4ed9d7bad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
            "[CV] END ..................C=2, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=2, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=2, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=2, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=2, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ................C=0.5, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ................C=0.5, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ................C=0.5, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ................C=0.5, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ................C=0.5, penalty=l2, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.0s\n",
            "[CV] END ..................C=1, penalty=l1, solver=liblinear; total time=   0.0s\n",
            "\n",
            "Best parameters after tuning are: LogisticRegression(C=2, max_iter=1000, solver='liblinear')\n",
            "Accuracy: 0.868421052631579\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=5)\n",
        "\n",
        "base_model = LogisticRegression(max_iter=1000)\n",
        "ovo_model = OneVsOneClassifier(base_model)\n",
        "\n",
        "ovo_model.fit(X_train, y_train)\n",
        "y_pred = ovo_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy Score (OvO):\", accuracy)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5x4-U40bV4oe",
        "outputId": "6b6ab8a4-e146-49a3-8a31-2ad92709db30"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score (OvO): 0.9473684210526315\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary classification.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "data= load_breast_cancer()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "accuracy= accuracy_score(y_test, y_pred)\n",
        "cm=confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(\"Accuracy Score:\", accuracy)\n",
        "print(\"\\nConfusion Matrix:\\n\" )\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Predicted 0', 'Predicted 1'],\n",
        "            yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        },
        "id": "EwyPRHnFXW8l",
        "outputId": "9881287e-85b5-4941-d791-4afa5cbd6e2b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Score: 0.9790209790209791\n",
            "\n",
            "Confusion Matrix:\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision, Recall, and F1-Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "\n",
        "data= load_breast_cancer()\n",
        "X= pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y= data.target\n",
        "\n",
        "scaler= StandardScaler()\n",
        "X= scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, random_state=5, test_size=0.25)\n",
        "\n",
        "model=LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred=model.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW17AjFoZyVm",
        "outputId": "901ab9f1-8bb4-40fd-b77e-1de939f7e89b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Confusion Matrix:\n",
            " [[53  2]\n",
            " [ 1 87]]\n",
            "\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.96      0.97        55\n",
            "           1       0.98      0.99      0.98        88\n",
            "\n",
            "    accuracy                           0.98       143\n",
            "   macro avg       0.98      0.98      0.98       143\n",
            "weighted avg       0.98      0.98      0.98       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to improve model performance.\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "X, y = make_classification(n_samples=10000, n_features=10,\n",
        "                           n_classes=2, weights=[0.99, 0.01],\n",
        "                            random_state=42)\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=22)\n",
        "\n",
        "model_plain = LogisticRegression(max_iter=1000)\n",
        "model_plain.fit(X_train, y_train)\n",
        "y_pred_plain = model_plain.predict(X_test)\n",
        "\n",
        "print(\"=== Without Class Weights ===\")\n",
        "print(classification_report(y_test, y_pred_plain))\n",
        "\n",
        "model_weighted = LogisticRegression(class_weight='balanced', max_iter=1000)\n",
        "model_weighted.fit(X_train, y_train)\n",
        "y_pred_weighted = model_weighted.predict(X_test)\n",
        "\n",
        "print(\"=== With class_weight='balanced' ===\")\n",
        "print(classification_report(y_test, y_pred_weighted))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLZ4a_ZLarNf",
        "outputId": "8341fb36-69e5-4912-f3dc-7a918f941e3f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Without Class Weights ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      2458\n",
            "           1       1.00      0.05      0.09        42\n",
            "\n",
            "    accuracy                           0.98      2500\n",
            "   macro avg       0.99      0.52      0.54      2500\n",
            "weighted avg       0.98      0.98      0.98      2500\n",
            "\n",
            "=== With class_weight='balanced' ===\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.80      0.88      2458\n",
            "           1       0.05      0.62      0.09        42\n",
            "\n",
            "    accuracy                           0.79      2500\n",
            "   macro avg       0.52      0.71      0.49      2500\n",
            "weighted avg       0.98      0.79      0.87      2500\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and evaluate performance.\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "df=sns.load_dataset(\"titanic\")\n",
        "print(df.sample(5))\n",
        "\n",
        "print()\n",
        "print(df.isnull().sum() )  # Check for missing values in the dataset\n",
        "\n",
        "#selecting important features which are relevant to model\n",
        "df=df[[\"pclass\",\"sex\",\"age\",\"fare\",\"alone\",\"survived\"]]\n",
        "print()\n",
        "print(df.info())\n",
        "\n",
        "# Imputing missing values for numerical variables as median\n",
        "df[\"age\"].fillna(df[\"age\"].median(), inplace=True)\n",
        "\n",
        "#Encoding\n",
        "ohe_encoder = OneHotEncoder(drop='first')\n",
        "ohe_encoded = ohe_encoder.fit_transform(df[[\"pclass\", \"sex\", \"alone\"]])  # Fit and transform the categorical features\n",
        "df_encoded = pd.DataFrame(ohe_encoded.toarray(), columns=ohe_encoder.get_feature_names_out())\n",
        "\n",
        "#Scaling\n",
        "scaler= StandardScaler()  # Create an instance of StandardScaler\n",
        "df_scaled = scaler.fit_transform(df[[\"age\", \"fare\"]])  # Fit and transform the numerical features\n",
        "df_scaled = pd.DataFrame(df_scaled, columns=[\"age\", \"fare\"])\n",
        "print()\n",
        "#Concatenating the final data frame\n",
        "df_final = pd.concat([df_encoded, df_scaled, df[\"survived\"]], axis=1)  # Combine the encoded and scaled features with the target variable\n",
        "print(\"Final Dataset:\\n\",df_final.head())\n",
        "\n",
        "#Training and testing the model\n",
        "X= df_final.drop(\"survived\", axis=1)  # Features\n",
        "y= df_final[\"survived\"]  # Target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split the data into training and testing sets\n",
        "log_reg = LogisticRegression()  # Create an instance of Logistic Regression\n",
        "log_reg.fit(X_train, y_train)  # Fit the model to the training data\n",
        "y_pred = log_reg.predict(X_test)  # Predict the target variable for the test set\n",
        "accuracy = accuracy_score(y_test, y_pred)  # Calculate accuracy\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)  # Calculate confusion matrix\n",
        "print()\n",
        "print(f'Accuracy: {accuracy:.2f}')\n",
        "print('Confusion Matrix:\\n', conf_matrix)\n",
        "print('Classification Report:\\n', classification_report(y_test, y_pred))  # Print classification report"
      ],
      "metadata": {
        "id": "K3a_jdb2ceky",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3522e14-e4b6-4f31-bebd-5808bfe8b550"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
            "429         1       3    male  32.0      0      0   8.0500        S   Third   \n",
            "179         0       3    male  36.0      0      0   0.0000        S   Third   \n",
            "9           1       2  female  14.0      1      0  30.0708        C  Second   \n",
            "773         0       3    male   NaN      0      0   7.2250        C   Third   \n",
            "152         0       3    male  55.5      0      0   8.0500        S   Third   \n",
            "\n",
            "       who  adult_male deck  embark_town alive  alone  \n",
            "429    man        True    E  Southampton   yes   True  \n",
            "179    man        True  NaN  Southampton    no   True  \n",
            "9    child       False  NaN    Cherbourg   yes  False  \n",
            "773    man        True  NaN    Cherbourg    no   True  \n",
            "152    man        True  NaN  Southampton    no   True  \n",
            "\n",
            "survived         0\n",
            "pclass           0\n",
            "sex              0\n",
            "age            177\n",
            "sibsp            0\n",
            "parch            0\n",
            "fare             0\n",
            "embarked         2\n",
            "class            0\n",
            "who              0\n",
            "adult_male       0\n",
            "deck           688\n",
            "embark_town      2\n",
            "alive            0\n",
            "alone            0\n",
            "dtype: int64\n",
            "\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 891 entries, 0 to 890\n",
            "Data columns (total 6 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   pclass    891 non-null    int64  \n",
            " 1   sex       891 non-null    object \n",
            " 2   age       714 non-null    float64\n",
            " 3   fare      891 non-null    float64\n",
            " 4   alone     891 non-null    bool   \n",
            " 5   survived  891 non-null    int64  \n",
            "dtypes: bool(1), float64(2), int64(2), object(1)\n",
            "memory usage: 35.8+ KB\n",
            "None\n",
            "\n",
            "Final Dataset:\n",
            "    pclass_2  pclass_3  sex_male  alone_True       age      fare  survived\n",
            "0       0.0       1.0       1.0         0.0 -0.565736 -0.502445         0\n",
            "1       0.0       0.0       0.0         0.0  0.663861  0.786845         1\n",
            "2       0.0       1.0       0.0         1.0 -0.258337 -0.488854         1\n",
            "3       0.0       0.0       0.0         0.0  0.433312  0.420730         1\n",
            "4       0.0       1.0       1.0         1.0  0.433312 -0.486337         0\n",
            "\n",
            "Accuracy: 0.80\n",
            "Confusion Matrix:\n",
            " [[91 14]\n",
            " [21 53]]\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.87      0.84       105\n",
            "           1       0.79      0.72      0.75        74\n",
            "\n",
            "    accuracy                           0.80       179\n",
            "   macro avg       0.80      0.79      0.80       179\n",
            "weighted avg       0.80      0.80      0.80       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "# model. Evaluate its accuracy and compare results with and without scaling.\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model_no_scaling = LogisticRegression(max_iter=1000)\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "acc_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy WITHOUT scaling:\", round(acc_no_scaling, 4))\n",
        "print(\"Accuracy WITH scaling   :\", round(acc_scaled, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKJ6g7AHetLA",
        "outputId": "ecabfafc-94f3-4f4c-9f2c-14ed2c3d53c8"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy WITHOUT scaling: 0.958\n",
            "Accuracy WITH scaling   : 0.979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "y_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "roc_auc = roc_auc_score(y_test, y_proba)\n",
        "print(\"ROC-AUC Score:\", round(roc_auc, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPdz_kVYfEEi",
        "outputId": "c024338c-9175-46d0-8d01-2a51adb8e0fc"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "c=0.5\n",
        "model = LogisticRegression(C=c, max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy with {c}:\", round(accuracy, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGYwnoL6gZqE",
        "outputId": "9e8d6f28-e1c6-40b3-c19a-a65d44df9f62"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with 0.5: 0.986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#18.  Write a Python program to train Logistic Regression and identify important features based on model coefficients.\n",
        "\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "coefs = model.coef_[0]\n",
        "feature_names = data.feature_names\n",
        "\n",
        "coef_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefs,\n",
        "    'Abs_Coefficient': np.abs(coefs)\n",
        "})\n",
        "\n",
        "coef_df.sort_values(by=\"Abs_Coefficient\" , ascending=False).head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "zIJvwFJ8iSCS",
        "outputId": "0807a1f7-73a9-47f4-f497-e18f5a74ce2e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Feature  Coefficient  Abs_Coefficient\n",
              "21         worst texture    -1.330051         1.330051\n",
              "10          radius error    -1.257580         1.257580\n",
              "28        worst symmetry    -1.230088         1.230088\n",
              "7    mean concave points    -1.071935         1.071935\n",
              "26       worst concavity    -1.006493         1.006493\n",
              "13            area error    -0.905674         0.905674\n",
              "20          worst radius    -0.839498         0.839498\n",
              "23            worst area    -0.815133         0.815133\n",
              "27  worst concave points    -0.794471         0.794471\n",
              "6         mean concavity    -0.754223         0.754223"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8b1977d-3266-4a7a-aae7-16ec794064ea\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Feature</th>\n",
              "      <th>Coefficient</th>\n",
              "      <th>Abs_Coefficient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>worst texture</td>\n",
              "      <td>-1.330051</td>\n",
              "      <td>1.330051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>radius error</td>\n",
              "      <td>-1.257580</td>\n",
              "      <td>1.257580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>worst symmetry</td>\n",
              "      <td>-1.230088</td>\n",
              "      <td>1.230088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>mean concave points</td>\n",
              "      <td>-1.071935</td>\n",
              "      <td>1.071935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>worst concavity</td>\n",
              "      <td>-1.006493</td>\n",
              "      <td>1.006493</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>area error</td>\n",
              "      <td>-0.905674</td>\n",
              "      <td>0.905674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>worst radius</td>\n",
              "      <td>-0.839498</td>\n",
              "      <td>0.839498</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>worst area</td>\n",
              "      <td>-0.815133</td>\n",
              "      <td>0.815133</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>worst concave points</td>\n",
              "      <td>-0.794471</td>\n",
              "      <td>0.794471</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>mean concavity</td>\n",
              "      <td>-0.754223</td>\n",
              "      <td>0.754223</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8b1977d-3266-4a7a-aae7-16ec794064ea')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e8b1977d-3266-4a7a-aae7-16ec794064ea button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e8b1977d-3266-4a7a-aae7-16ec794064ea');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-2c664dd4-03f4-40de-b1e8-e52d72002ff5\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2c664dd4-03f4-40de-b1e8-e52d72002ff5')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-2c664dd4-03f4-40de-b1e8-e52d72002ff5 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"coef_df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"Feature\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"worst concave points\",\n          \"radius error\",\n          \"area error\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Coefficient\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21211049379228047,\n        \"min\": -1.330050638029533,\n        \"max\": -0.7542227948780492,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          -0.7944712716944323,\n          -1.2575797841079723,\n          -0.9056736244277362\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Abs_Coefficient\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.21211049379228047,\n        \"min\": 0.7542227948780492,\n        \"max\": 1.330050638029533,\n        \"num_unique_values\": 10,\n        \"samples\": [\n          0.7944712716944323,\n          1.2575797841079723,\n          0.9056736244277362\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Write a Python program to train Logistic Regression and evaluate its performance using Cohens Kappa Score.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "print(\"Cohen's Kappa Score:\", round(kappa_score, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kKlzLUtjCo5",
        "outputId": "5df5c72e-412c-419a-c9af-2b6176c0410f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.9555\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary classification.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_scores = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_scores)\n",
        "avg_precision = average_precision_score(y_test, y_scores)\n",
        "\n",
        "# Plot the curve\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(recall, precision, label=f\"Logistic Regression (AP = {avg_precision:.2f})\", color=\"blue\")\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "kA6G141HkINX",
        "outputId": "e7eccb10-e2cc-4898-fe7c-66b27a073209"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare their accuracy.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        model = LogisticRegression(solver=solver, max_iter=1000)\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        results[solver] = round(acc, 5)\n",
        "    except Exception as e:\n",
        "        results[solver] = f\"Failed: {e}\"\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy by Solver:\")\n",
        "for solver, score in results.items():\n",
        "    print(f\"{solver:10} : {score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQjLYwBFkW0L",
        "outputId": "29361b13-b31d-441c-8966-1cd598417aa1"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy by Solver:\n",
            "liblinear  : 0.97902\n",
            "saga       : 0.97902\n",
            "lbfgs      : 0.97902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews Correlation Coefficient (MCC).\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"Matthews Correlation Coefficient:\", round(mcc, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyXvPlBVlgxz",
        "outputId": "736f6076-0984-4eb6-bd64-d9fecda88c1c"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient: 0.9556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their accuracy to see the impact of feature scaling.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model_raw = LogisticRegression(max_iter=1000)\n",
        "model_raw.fit(X_train_raw, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test_raw)\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_raw)\n",
        "X_test_scaled = scaler.transform(X_test_raw)\n",
        "\n",
        "model_scaled = LogisticRegression(max_iter=1000)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(\"Accuracy on raw data:\", round(acc_raw, 4))\n",
        "print(\"Accuracy on standardized data:\", round(acc_scaled, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcnuuAZglxN1",
        "outputId": "08c24e70-af29-4e81-a619-daf7c951a648"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on raw data: 0.958\n",
            "Accuracy on standardized data: 0.979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using cross-validation.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n",
        "C_values = [0.01, 0.1, 0.5, 1, 2, 5, 10]\n",
        "best_score = 0\n",
        "best_C = None\n",
        "\n",
        "for C in C_values:\n",
        "    model = LogisticRegression(C=C, max_iter=1000)\n",
        "    scores = cross_val_score(model, X_train, y_train, cv=5)\n",
        "    avg_score = np.mean(scores)\n",
        "    if avg_score > best_score:\n",
        "        best_score = avg_score\n",
        "        best_C = C\n",
        "\n",
        "final_model = LogisticRegression(C=best_C, max_iter=1000)\n",
        "final_model.fit(X_train, y_train)\n",
        "test_score = final_model.score(X_test, y_test)\n",
        "\n",
        "print(\"Best C value:\", best_C)\n",
        "print(\"Cross-validated Accuracy:\", round(best_score, 4))\n",
        "print(\"Test Accuracy with best C:\", round(test_score, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlT9zOLhmMJs",
        "outputId": "1e7bb128-2204-410c-97a7-5119872d2c1b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value: 5\n",
            "Cross-validated Accuracy: 0.9788\n",
            "Test Accuracy with best C: 0.972\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to make predictions.\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.25, random_state=42)\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(model, \"logistic_model.joblib\")\n",
        "\n",
        "loaded_model = joblib.load(\"logistic_model.joblib\")\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy after loading model:\", round(accuracy, 4))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BD61_Mi9m1wV",
        "outputId": "2f042f95-207b-4df3-d3ad-7a59c5848ba4"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy after loading model: 0.979\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yowdnuX8nlxZ"
      },
      "execution_count": 35,
      "outputs": []
    }
  ]
}